\documentclass[a4paper]{jpconf}
\usepackage{graphicx}
\usepackage{lineno}
\linenumbers
\begin{document}
\title{Deployment of IPv6-only CPU resources at WLCG sites}

%  All 15-minute Oral Presentations and Poster Presentations have a
%  limit of 8 pages.

\author{M Babik$^1$, J Chudoba$^2$, A Dewhurst$^3$, A Sciab\`a$^1$, S
  Fayer$^9$, T Finnern$^7$, T Froy$^6$, C Grigoras$^1$, K Hafeez$^3$,
  B Hoeft$^4$, T Idiculla$^3$, D Kelsey$^3$, E Martelli$^1$, F
  Munoz$^{10}$, R Nandakumar$^3$, K Ohrenberg$^6$, F Prelz$^5$, D
  Rand$^9$, U Tigerstedt$^{11}$, D Traynor$^6$ and R Voicu$^8$}

\address{$^1$ CERN, CH-1211 Gen\`{e}ve 23, Switzerland}
\address{$^2$ Institute of Physics, Academy of Sciences of the Czech
  Republic Na Slovance 2 182 21 Prague 8, Czech Republic}
\address{$^3$ STFC - Rutherford Appleton Lab. UK}
\address{$^4$ Karlsruher Institut f\"{u}r Technologie,
  Hermann-von-Helmholtz-Platz 1, D-76344 Eggenstein-Leopoldshafen,
  Germany}
\address{$^5$ INFN, Sezione di Milano, via G. Celoria 16, I-20133
  Milano, Italy}
\address{$^6$ Queen Mary University of London, Mile End Road, London
  E1 4NS, United Kingdom}
\address{$^7$ Deutsches Elektronen-Synchrotron, Notkestra{\ss}e 85,
  D-22607 Hamburg, Germany}
\address{$^8$ California Institute of Technology, Pasadena, Ca 91125,
  U.S.A.}
\address{$^9$ Imperial College London, South Kensington Campus, London
  SW7 2AZ, United Kingdom}
\address{$^{10}$ Port dâ€™Informaci\'{o} Cient\'{i}fica (PIC),
  Universitat Aut\`{o}noma de Barcelona, Bellaterra (Barcelona),
  Spain}
\address{$^{11}$ CSC Tieteen Tietotekniikan Keskus Oy, P.O. Box 405,
  FI-02101 Espoo, Finland}
\ead{alastair.dewhurst@cern.ch, ipv6@hepix.org}

\begin{abstract}
The fraction of Internet traffic carried over IPv6 continues to grow
rapidly. IPv6 support from network hardware vendors and carriers is
pervasive and becoming mature. A network infrastructure upgrade often
offers sites an excellent window of opportunity to configure and
enable IPv6.

There is a significant overhead when setting up and maintaining
dual-stack machines, so where possible sites would like to upgrade
their services directly to IPv6 only. In doing so, they are also
expediting the transition process towards its desired
completion. While the LHC experiments accept there is a need to move
to IPv6, it is currently not directly affecting their work. Sites are
unwilling to upgrade if they will be unable to run LHC experiment
workflows. This has resulted in a very slow uptake of IPv6 from WLCG
sites.

For several years the HEPiX IPv6 Working Group has been testing a
range of WLCG services to ensure they are IPv6 compliant. Several
sites are now running many of their services as dual-stack. The
working group, driven by the requirements of the LHC VOs to be able to
use IPv6-only opportunistic resources, continues to encourage wider
deployment of dual-stack services to make the use of such IPv6-only
clients viable.

This paper will present the HEPiX plan and progress so far to allow
sites to deploy IPv6-only CPU resources. This will include making
experiment central services dual-stack as well as a number of storage
services. The monitoring, accounting and information services that are
used by jobs also needs to be upgraded. Finally the VO testing that
has taken place on hosts connected via IPv6-only will be reported.
\end{abstract}

\section{Introduction}
The fraction of internet traffic carried over IPv6 continues to grow
rapidly. Over one eighth of queries to google traffic go via
IPv6\cite{GoogleIPv6}. Apple recently announced\cite{ApplePolicy} that
all Apps produced for their products must be able to work over
IPv6-only networks. Large cloud providers such as
Amazon\cite{AmazonIPv6} and Microsoft\cite{Microsoft} provision
dual-stack machines, while some smaller cloud
providers\cite{CheapIPv6} will offer cheaper VMs if they are
IPv6-only. Within the HEP community there are over 10 sites that have
deployed dual-stack storage, while others have expressed a desire to
deploy IPv6-only WNs. Not only does IPv6 provide a solution to the
limited number of IPv4 address it also offers potential benefits.
e.g., for security conscious sites, it is possible to assign every job
that runs on their batch system a unique IPV6 address, this means that
if suspicious behaviour is detected it becomes significant easier to
trace the source.

\section{IPv6-only CPU resources}
Despite the advantages of IPv6 and the ever increasing deployment
across the world, deployment at WLCG sites has remained slow.  The
following reasons were identified for this.
\begin{itemize}
\item No appetite from the LHC VOs.  The ability to access data and
  run analysis jobs had not been a problem, so from their point of
  view, why change?
\item There is an initial cost (primarily manpower) to setup IPv6 at a
  site as well as a small ongoing overhead running dual-stack
  services.
\item IPv4 address exhaustion was not affecting several of the larger
  WLCG sites (Tier 1s) who often lead when it comes to adopting new
  technologies.
\end{itemize}

The WLCG is expected to evolve under the assumption of flat cash
funding for computing resources and it is therefore important that
sites are not hindered in their procurement by unnecessary
restrictions from the WLCG VOs. Hardware procurements often have a
significant lead time and will often be in production for several
years.  Even if a site does not intend to switch to IPv6 any time
soon, they may well be making procurement decisions now which will
influence their decision to migrate some time in the next 5 years.

The HEPiX WG came to the conclusion that the only way to ensure that
IPv6 adoption did not become a problem was to make a strategic
decision to mandate the LHC VOs as well as all Tier 1 sites to provide
a minimum level of IPv6 support.  This would allow any other site to
provide IPv6 support with confidence that support would be available
in the event of problems.

In order to provide an incentive for sites to move it was also decided
that any agreement must allow sites to completely migrate some
services to IPv6. Sites traditionally provide both storage and CPU.
The current LHC Computing models allow transfers between any two sites
and therefore for IPv6-only storage to be supported, all sites would
need to provide dual-stack storage.  For CPU, they traditionally only
talk to their internal sites services as well as a handful of VO boxes
for the VO running the jobs.  It was therefore decided that it was
easier to allow CPU resources to be migrated completely to IPv6.

In July 2016 the HEPiX Working group submitted a proposal to the WLCG
Management Board setting out a plan to allow sites, if they so choose,
to deploy their CPU resources as IPv6-only.

\section{IPv6 Peering and PerfSonar work}
In the past the HEPiX IPv6 working group has requested that sites
offer IPv6 peering over the LHCOPN.
\begin{itemize}
\item All Tier-1s to offer IPv6 peering to LHCOPN and provide
  dual-stack PerfSONAR machine by April 2015.
\item All Tier-2s to offer IPv6 peering to LHCONE and provide
  dual-stack PerfSONAR machine by August 2015.
\end{itemize}
As this was a request from a working group rather than a mandate from
the WLCG take up was not full.  In the month since the WLCG agreement
was approved, two additional Tier 1s (BNL and Triumf) are now peering
over the LHCOPN.

\section{Software validation}
The HEPiX IPv6 working group has also invested a significant amount of
time in validating software as IPV6 ready. Key storage software and
protocols work: dCache, DPM, StoRM XrootD 4, GridFTP, http.

\section{Central service migration}
\subsection{CVMFS}
All the WLCG VOs as well as many others distribute their software
across the Grid using CVMFS. The software is uploaded to a Stratum-0
server (located at CERN for the WLCG VOs) which then mirrors the data
to several Stratum-1 servers\cite{Stratum1}.  Jobs will access the VO
software from a cache on the local disk; if the file is not available,
it will be looked for in the site Squid server, which in turn, will
contact a Stratum-1 if needed. Squid 3.x is IPv6 compliant and is
being used in production by some sites.  It is essential that the
Stratum-1 service at CERN is upgraded to dual-stack by April 2017.
When possible the Tier 1 should upgrade their service to dual-stack
and all Tier 1s should be upgraded by April 2018 at the very latest.

\subsection{FTS}
ATLAS, CMS and LHCb all use the FTS service extensive for data
movement around the Grid. Jobs do not contact the FTS service directly
so it is not necessary for the FTS service to be dual-stack. All VOs
are encouraging sites to make their storage dual-stack. Transfers via
two dual-stack service should go via IPv6, however it is the FTS
server which initiates the negotiation and sends a PASV (on IPv4) or
an EPSV (on IPv6) to the destination and sends the IP (for the
corresponding protocol) and port to the source.  Therefore all FTS
services should be upgraded to allow transfers between dual-stack
sites to go over IPv6.

Currently the FTS service at CERN is dual-stack. There are IPv4-only
FTS services at RAL, BNL and Fermilab that are used by the LHC VOs.
While it is possible to work around this all FTS services should be
upgraded to dual-stack when possible and by April 2018 at the very
latest.

\subsection{PerfSonar}
PerfSonar instances are required at all WLCG sites to implement the
network monitoring infrastructure. All Tier-1s were requested to
provide a dual-stack perfSonar instance and GGUS tickets have now been
submitted to those that have not. PerfSonar is a very good way of
checking that the migration to IPv6 hasn't caused any network/routing
problems. All sites are requested to provide a dual-stack PerfSonar
instance by April 2018 at the latest. While it is not essential for
all Tier 2s to migrate, it would be concerning if they are unable to
provide a PerfSonar instance by this time. Any site unable to provide
a PerfSonar instance by April 2018 will be requested to provide a
clear description of their IPv6 plans.

\subsection{ETF test infrastructure}
A separate IPv6-only ETF test infrastructure will need to be set up to
monitor IPv6-ready sites. This must be done by April 2017.  This will
be run in parallel to the production ETF test infrastructure. This
service will provide sites with low level monitoring to help them
identify problems with their IPv6 migration and not used for official
availability metrics unless the site is providing some resources on
IPv6-only. From April 2018 the official ETF infrastructure will be
migrated to dual-stack. From this point on production work going over
IPv6 should be considered entirely normal. This will hopefully
encourage sites to investigate IPv6 before April 2018.

\subsection{Frontier Service}
ATLAS and CMS both use the Frontier Service\cite{Frontier} to access
conditions data across the Grid. The Frontier service has three
components:
\begin{itemize}
\item Frontier client: This software is run by ATLAS and CMS jobs. It
  converts a conditions database query into an HTTP request. The
  Frontier Client was made IPV6 compliant in January 2016.
\item Squid proxy: Sites are expected to deploy squid servers to cache
  the conditions data requests.
\item Frontier Launchpad: This converts the HTTP requests back into
  database queries which are then submitted to the conditions
  database.
\end{itemize}

\subsection{Other Services}
There are several other services such as certificate authorities,
software repositories, the GOCDB/OIM, GGUS, VOMS and the BDII. These
are not used directly by jobs but are needed when configuring the
site. These services should be made dual-stack when possible and
ideally by April 2018 (although some services might not fall under the
WLCG banner). It will depend heavily on the site setup as to whether
the lack of IPv6 connectivity will cause problems. Problems will have
to be followed up by the HEPiX working group as they appear.


\section{VO migration}
The following section details the ongoing work from the LHC VOs to
upgrade their services to allow jobs to run on IPv6-only CPU
resources.

\subsection{ALICE}
Unlike the other LHC VOs, ALICE uses fully federated storage, any site
can access the storage element of another site if needed (reading,
writing and data transfers). Therefore in order to ensure all job
types can run on IPv6-only CPU all data needs to be accessible over
IPv6. Some data is stored on multiple sites and therefore it does not
necessarily mean all sites will need to be dual-stack. To support
IPv6, the site storage elements need to run xrootd v.4. The central
ALICE Grid services have been tested to run on IPv6 and are running in
dual-stack mode for over a year. For sites supporting ALICE the
current situation is:
\begin{itemize}
\item One third of the sites are still running SEs with xrootd v.3.
\item $5\%$ of the SEs are running in dual-stack mode, while the
  remaining are IPv4.
\end{itemize}

\subsection{ATLAS}
The ATLAS workload management system is called PanDA \cite{Panda}.
Pilot factories generate pilot jobs which are sent directly to CEs at
sites. Once these pilots are started by the batch system, they will
contact a central Panda Server to pull in a job (done via http). They
will also contact the Rucio Server for File lookup (done via http) and
the local storage. Some ATLAS jobs access Conditions data using the
Frontier service. At the end of the job the pilot will write the
output files to a local SE. Every 30 minutes while the job is running
the pilot will report to the Panda server (via http). It will also
contact the Panda Server at the end of the job. ATLAS jobs running on
IPv6 WN will need access to the following resources:
\begin{itemize}
\item The production panda server nodes
\item The Rucio Authentication nodes
\item The Rucio Production nodes
\item The Frontier servers at CERN, IN2P3, RAL and Triumf
\end{itemize}  
The pilot factories that submit jobs to CEs have been made dual-stack.
ATLAS also use the ARC Control Tower (aCT) to submit jobs primarily to
NorduGrid but potentially any sites running an ARC CE. This will also
need to be made dual-stack. ATLAS are working on making all these
services dual-stack by April 2017.

\subsection{CMS}
The job submission middleware, glideinWMS, is used to launch HTCondor
worker nodes and its major components (frontend and factory). These
have been validated as IPv6-compliant. Some glidein factories are
already deployed in dual-stack. HTCondor itself is fully
IPv6-compliant, but the collectors and schedds still need to be all
dual-stack in production in order to support IPv6-only worker nodes.
The glideinWMS Integration Testbed (ITB) has been configured to be fully
dual-stack and it is used to test job submission to a few sites which
have enabled IPv6 on their WNs.

The central services hub, cmsweb.cern.ch, has been validated for
dual-stack operation and its production instance is running in
dual-stack mode. The CMS-specific job management systems (WMAgent for
production and CRAB3 for analysis) have not yet been fully tested on
IPv6, but they are expected to work with little effort needed. In any
case, they do not need to be in dual-stack for the foreseeable future,
as they interact only with other central services.

The data management system, PhEDEx, uses the Oracle client for
communication between local site agents and the central service. Tests
have not yet been done, but Oracle 12c fully supports IPv6. Dual-stack
operation will not be required anyway.

Concerning AAA, the CMS storage federation, only a very small fraction
of the data is accessible using xrootd or GridFTP via IPv6. The global
and regional redirectors are only partly on dual-stack.

CMS plans to immediately start upgrading all central services to
dual-stack, to be completed in principle by the end of Run II, but
likely much earlier. Services contacted by worker nodes (like
HTCondor) will be given priority and will aim to be done by April
2017. A campaign will be launched at the beginning of 2017 to
encourage Tier-2 sites to upgrade their storage systems to dual-stack.

At the time of writing, only eleven CMS sites expose IPv6 addresses
for their services.

\subsection{LHCb}
LHCb uses the DIRAC framework to submit jobs to the grid. DIRAC
officially supports IPv6 and some other VOs, who use DIRAC, are
already using a dual-stack service in production.  LHCb submits
generic pilot jobs to CEs as needed. When these pilots start on a WN,
they contact the LHCb DIRAC central services for available tasks (via
the dips protocol) which are then executed. If input data is needed,
they contact the relevant storages using the sites SRM\footnote{The
  job is given a list of locations of the input files by DIRAC. It
  currently contacts the site SRMs in turn to retrieve the data. This
  will in future be updated to bypass the SRM and construct the file
  location automatically using the information available.} to access
the data.  Production jobs typically retrieve / download the data to
the worker node, as they know exactly how much data is needed. User
jobs stream data from the storage directly.

Once the job is done, it will upload the output to a storage
location. If the default preferred location is not available, all
other possible locations (available for LHCb) are tried in turn until
successful and a request is set in the central services of LHCb to
transfer the file to the preferred location when possible. If no
location is available, the job ends up in status "failed", and could
be resubmitted depending on the conditions.

LHCb jobs running on an IPv6-only WN will need access to the following
resources:
\begin{itemize}
\item LHCb's DIRAC central services
\item Storage services supporting LHCb
\item Optionally, one of six VO-boxes at LHCb Tier-1 sites
\end{itemize}
Currently there is one Tier-1 storage and one Tier-2D storage that
support LHCb in a dual-stack configuration.  The LHCb central services
are being moved to dual-stack machines.


\section{Conclusions}
The LHC VOs are committed to being able to work on the Grid over IPv6.
Much work still remains to be done to make this a reality.  The HEPiX
IPv6 working group has validated that all essential software is IPv6
compliant.  Software developers should consider IPv6 compliance a
standard requirement and the emphasis should be on them to test this.
All the VOs have analysed their workflows on the Grid and have
provided a list of services which they will need to make dual-stack.
While exact time lines have not been agreed the amount of work
required is sufficiently small that it should be achievable by April
2017 without significantly disrupting normal WLCG operations.

From April 2017 sites will be allowed to deploy IPv6-only CPU
resources. Sites wishing to deploy IPv6-only CPU must deploy
dual-stack storage if they provide it.  All sites are encouraged to
upgrade their storage to dual-stack.  From the contact the HEPiX IPv6
working group has with sites, we believe that there are at most one or
two sites that wish to urgently upgrade making up less than $2\%$ of
the pledged WLCG CPU resources.  Any site wishing to upgrade should be
in contact with the HEPiX IPv6 working group to ensure that the
inevitable teething problems are resolved promptly.  By April 2018 it
should be possible to deploy IPv6-only CPU resources with relative
ease and by the end of Run II enough sites should have upgraded their
storage to dual-stack to allow almost complete data availability via
federated XrootD over IPv6.


%\subsection{Acknowledgments}



\section*{References}
\bibitem{CheapIPv6} https://www.mythic-beasts.com/servers/virtual

\bibitem{ApplePolicy} https://developer.apple.com/news/?id=05042016a

\bibitem{Panda} https://twiki.cern.ch/twiki/bin/view/PanDA/PanDA

\bibitem{Stratum1} http://cernvm-monitor.cern.ch/cvmfs-monitor/atlas.cern.ch/

\bibitem{Frontier} http://frontier.cern.ch/

\end{thebibliography}

\end{document}


